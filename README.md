# Deductive Coding with Large Language Models

This project explores the application of advanced Large Language Models (LLMs) in deductive coding tasks, a cornerstone of qualitative research. By leveraging state-of-the-art LLMs such as Llama 3 series and GPT-3.5, the study aims to evaluate their performance across diverse datasets and coding frameworks.

---

## Abstract

Deductive coding, though invaluable for systematic text analysis, is time-intensive and labor-intensive. This project benchmarks modern LLMs in replicating and enhancing human-coded frameworks, with a particular focus on:

- Comparing model performance across datasets of varying complexity.
- Evaluating the impacts of prompt engineering techniques, such as Chain-of-Thought (CoT) and decomposition.
- Highlighting trade-offs between model size, computational cost, and accuracy.

Persistent challenges include hierarchical coding, overlapping categories, and domain-specific nuances. The findings emphasize the potential of LLMs to streamline deductive coding while pointing to areas for improvement.

---

## Repository Contents

### Main Files
- **`agenttest.`**: General testing of model agents - Not finished
- **`bbc_news`**: Analysis of BBC news dataset.
- **`contraclaims.`**: Analysis of contrarian claims dataset.
- **`trump_tweet.`**: Exploration of Trump Tweets dataset.
- **`ukrainewater.`**: Analysis of Ukraine water problems dataset.

### Results and Data
- **`results_csvs/`**: Output CSVs summarizing model performance and alignment.
- **`Data/`**: Raw datasets used for analysis.

### Additional Files
- **`.gitignore`**: Git ignore rules.
- **`LICENSE`**: Project licensing information.
- **`README.md`**: Project overview.


